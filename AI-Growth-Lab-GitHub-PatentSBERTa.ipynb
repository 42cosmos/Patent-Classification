{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of the program is 62.681822299957275\n",
      "F1Measure:  0.0\n",
      "Recall:  0.0\n",
      "Accuracy:  0.0\n",
      "Precision:  0.0\n",
      "Hamming_Loss:  0.002870090634441088\n",
      "Runtime of the program is 66.3840844631195\n",
      "F1Measure:  0.0\n",
      "Recall:  0.0\n",
      "Accuracy:  0.0\n",
      "Precision:  0.0\n",
      "Hamming_Loss:  0.002870090634441088\n",
      "Runtime of the program is 71.70731830596924\n",
      "F1Measure:  0.0\n",
      "Recall:  0.0\n",
      "Accuracy:  0.0\n",
      "Precision:  0.0\n",
      "Hamming_Loss:  0.002870090634441088\n",
      "Runtime of the program is 78.95103883743286\n",
      "F1Measure:  0.4888888888888887\n",
      "Recall:  0.5716666666666667\n",
      "Accuracy:  0.44041666666666673\n",
      "Precision:  0.45233333333333337\n",
      "Hamming_Loss:  0.001948640483383686\n",
      "Runtime of the program is 88.63805508613586\n",
      "F1Measure:  0.5997936507936505\n",
      "Recall:  0.6741666666666666\n",
      "Accuracy:  0.5353690476190477\n",
      "Precision:  0.5822857142857144\n",
      "Hamming_Loss:  0.0018126888217522659\n",
      "Runtime of the program is 99.94861054420471\n",
      "F1Measure:  0.6331587301587299\n",
      "Recall:  0.6936666666666668\n",
      "Accuracy:  0.5622857142857144\n",
      "Precision:  0.6426190476190476\n",
      "Hamming_Loss:  0.001933534743202417\n",
      "Runtime of the program is 112.62773489952087\n",
      "F1Measure:  0.6407301587301584\n",
      "Recall:  0.6733333333333333\n",
      "Accuracy:  0.5592857142857143\n",
      "Precision:  0.6762857142857144\n",
      "Hamming_Loss:  0.0021299093655589123\n",
      "Runtime of the program is 127.30106163024902\n",
      "F1Measure:  0.6519682539682538\n",
      "Recall:  0.6686666666666667\n",
      "Accuracy:  0.5687857142857144\n",
      "Precision:  0.7071190476190476\n",
      "Hamming_Loss:  0.0021601208459214503\n",
      "Runtime of the program is 143.7013120651245\n",
      "F1Measure:  0.6589523809523805\n",
      "Recall:  0.6621904761904761\n",
      "Accuracy:  0.5687857142857142\n",
      "Precision:  0.7349523809523809\n",
      "Hamming_Loss:  0.002175226586102719\n",
      "Runtime of the program is 162.19792413711548\n",
      "F1Measure:  0.6525374625374621\n",
      "Recall:  0.6483333333333333\n",
      "Accuracy:  0.5590079365079366\n",
      "Precision:  0.7416190476190475\n",
      "Hamming_Loss:  0.0022809667673716013\n",
      "Runtime of the program is 182.38425588607788\n",
      "F1Measure:  0.64844511044511\n",
      "Recall:  0.6356904761904761\n",
      "Accuracy:  0.5499126984126983\n",
      "Precision:  0.7536190476190475\n",
      "Hamming_Loss:  0.0024773413897280967\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# get_ipython().system('pip install sentence_transformers')\n",
    "import scipy.spatial\n",
    "import numpy as np\n",
    "import os, json\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tokenizers import Tokenizer\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "F1Measure_list = []\n",
    "Recall_list = []\n",
    "Accuracy_list = []\n",
    "Precision_list = []\n",
    "Hamming_Loss_list = []\n",
    "\n",
    "\n",
    "#Load sentences & embeddings from disc\n",
    "with open('/home/ubuntu/storage_data_new/df_claim_test_1M_pre_duplicates_removed_663.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_patent_test_embeddings_id = stored_data['patent_id']\n",
    "    test_embeddings = stored_data['claim_embeddings']\n",
    "\n",
    "with open('/home/ubuntu/storage_data_new/df_claim_train_1M_pre_duplicates_removed_663.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_patent_train_embeddings_id = stored_data['patent_id']\n",
    "    claim_embeddings = stored_data['claim_embeddings']\n",
    "\n",
    "# test_embeddings = torch.load('/home/ubuntu/storage_data/df_claim_test_133_n.pkl', map_location=torch.device('cpu'))\n",
    "# claim_embeddings = torch.load('/home/ubuntu/storage_data/df_claim_train_133_n.pkl', map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "def get_top_n_similar_patents_df(new_claim, claim_embeddings):\n",
    "    search_hits_list = []\n",
    "    search_hits = util.semantic_search(new_claim, claim_embeddings, 10000, 5000000, 20)\n",
    "#     # embedder = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
    "#     embedder = SentenceTransformer('/home/ubuntu/deeppatentsimilarity/results/stsb_augsbert_SS_roberta-base-2021-01-06_22-14-54')\n",
    "# #     embedder = SentenceTransformer('roberta-base-nli-stsb-mean-tokens')\n",
    "# #     embedder = SentenceTransformer('/home/ubuntu/deeppatentsimilarity/results/stsb_augsbert_SS_roberta-base-2021-01-06_22-14-54/')\n",
    "# #     query_embeddings = embedder.encode([new_claim])\n",
    "#     query_embeddings = new_claim\n",
    "    \n",
    "# #     query_embeddings = torch.load('/home/ubuntu/deeppatentsimilarity/patentdata/claim_embeddings_24_AugSBERT_test.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "# #     query_embeddings = tokenizer([new_claim], padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#     # list of patent claims\n",
    "# #     claim_embeddings = embedder.encode(claims)\n",
    "# #     claim_embeddings = torch.load('/home/ubuntu/deeppatentsimilarity/patentdata/claim_embeddings_24_AugSBERT.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "#     # get top 100 patent claims based on cosine similarity\n",
    "#     top_n = 40\n",
    "#     distances = scipy.spatial.distance.cdist(query_embeddings, claim_embeddings, \"cosine\")[0]\n",
    "# #     distances = sentence_transformers.util.semantic_search(query_embeddings, claim_embeddings, \"cosine\")[0]\n",
    "\n",
    "#     results = zip(range(len(distances)), distances)\n",
    "#     results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    # save similar patents info\n",
    "    top_claim_order = []\n",
    "    top_claim_ids = []\n",
    "    top_similarity_scores = []\n",
    "    \n",
    "#     # print('New_claim: ' + new_claim + '\\n')\n",
    "\n",
    "#     # Find the closest 100 patent claims for each new_claim based on cosine similarity\n",
    "#     for idx, distance in results[0:top_n]:\n",
    "#         top_claim_ids.append(patent_id[idx])\n",
    "#         top_claims.append(claims[idx])\n",
    "#         top_similarity_scores.append(round((1-distance), 4))\n",
    "# #         print('Patent ID: ' + str(patent_id[idx]))\n",
    "# #         print('PubMed Claim: ' + claims[idx])\n",
    "# #         print('Similarity Score: ' + \"%.4f\" % (1-distance))\n",
    "# #         print('\\n')\n",
    "        \n",
    "    for item in range(len(search_hits[0])):\n",
    "        top_claim_order = search_hits[0][item].get('corpus_id')\n",
    "        top_claim_ids.append(stored_patent_train_embeddings_id[top_claim_order])\n",
    "        top_similarity_scores.append(search_hits[0][item].get('score'))\n",
    "        \n",
    "    top_100_similar_patents_df = pd.DataFrame({\n",
    "        'top_claim_ids': top_claim_ids,\n",
    "        'cosine_similarity': top_similarity_scores,\n",
    "#         'claims': top_claims,\n",
    "    })\n",
    "\n",
    "        \n",
    "    \n",
    "    return top_100_similar_patents_df\n",
    "\n",
    "def F1Measure(y_true, y_pred):\n",
    "    save_F1 = []\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if (sum(y_true[i]) == 0) and (sum(y_pred[i]) == 0):\n",
    "            continue\n",
    "        temp_save = (2*sum(np.logical_and(y_true[i], y_pred[i])))/ (sum(y_true[i])+sum(y_pred[i]))\n",
    "        save_F1.append(temp_save)\n",
    "        temp += temp_save\n",
    "\n",
    "    save_F1 = pd.DataFrame(save_F1)\n",
    "    save_F1_ids = pd.concat([result, save_F1], axis=1, ignore_index=True)\n",
    "#     save_F1_ids.to_csv(r'/home/ubuntu/storage_data/output/output_save_F1'+'-'+str(k)+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")+'.csv', index = False)\n",
    "    return temp/ y_true.shape[0]\n",
    "\n",
    "def Recall(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if sum(y_pred[i]) == 0:\n",
    "            continue\n",
    "        temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_pred[i])\n",
    "    return temp/ y_true.shape[0]\n",
    "\n",
    "def Precision(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if sum(y_true[i]) == 0:\n",
    "            continue\n",
    "        temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_true[i])\n",
    "    return temp/ y_true.shape[0]\n",
    "\n",
    "def Hamming_Loss(y_true, y_pred):\n",
    "    temp=0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        temp += np.size(y_true[i] == y_pred[i]) - np.count_nonzero(y_true[i] == y_pred[i])\n",
    "    return temp/(y_true.shape[0] * y_true.shape[1])\n",
    "\n",
    "def Accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "df_claim_cpc_test = pd.read_csv('/home/ubuntu/storage_data_new/df_claim_test_1M_pre_duplicates_removed_663.csv', encoding='ISO-8859-1')\n",
    "df_claim_cpc_train = pd.read_csv('/home/ubuntu/storage_data_new/df_claim_train_1M_pre_duplicates_removed_663.csv', encoding='ISO-8859-1')\n",
    "# df_claim_cpc_train_1000 = pd.read_csv('/home/ubuntu/deeppatentsimilarity/patentdata/df_claim_cpc_all_len_150_200_1000.csv', encoding='ISO-8859-1')\n",
    "# df_claim_cpc_test = pd.read_csv('/home/ubuntu/deeppatentsimilarity/patentdata/prelabel/NewTest/df_1_L_43259_test_100.csv', encoding='ISO-8859-1')\n",
    "# df_claim_cpc_train = pd.read_csv('/hsxome/ubuntu/deeppatentsimilarity/patentdata/prelabel/NewTest/df_1_L_43259.csv', encoding='ISO-8859-1')\n",
    "\n",
    "claims = list(df_claim_cpc_train.text)\n",
    "patent_id = list(df_claim_cpc_train.id)\n",
    "\n",
    "listofpredictdfs = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(df_claim_cpc_test[:100])):\n",
    "    get_top_n_similar_patents_df_predict = get_top_n_similar_patents_df(np.array(test_embeddings[i]).reshape(1,-1), claim_embeddings)\n",
    "    result = pd.merge(get_top_n_similar_patents_df_predict, df_claim_cpc_train, left_on='top_claim_ids',right_on='id',how='left',suffixes=('_left','_right'))\n",
    "    locals()[\"predict_n\"+str(i)] = result.copy()\n",
    "    listofpredictdfs.append(\"predict_n\"+str(i))\n",
    "\n",
    "# merged = list(itertools.chain(*listofpredictdfs))\n",
    "# df_names = pd.read_csv('/home/ubuntu/deeppatentsimilarity/patentdata/df_names.csv', encoding='ISO-8859-1')\n",
    "# df_names = df_names.values.tolist()\n",
    "# df_names..to_list()\n",
    " \n",
    "# df = pd.concat(listofpredictdfs,keys= listofpredictdfs ,axis=0).reset_index(level=listofpredictdfs)\n",
    "df = pd.concat(map(lambda x: eval(x), listofpredictdfs),keys= listofpredictdfs ,axis=0)\n",
    "\n",
    "top_k = 20\n",
    "for k in range(top_k):\n",
    "    top_n = k\n",
    "    predict = pd.DataFrame(columns= df_claim_cpc_test.columns[6:])\n",
    "    for item in range(len(listofpredictdfs)):\n",
    "        k_similar_patents = df.xs(listofpredictdfs[item]).nlargest(top_n, ['cosine_similarity'])\n",
    "        result_k_similar_patents = pd.DataFrame(0, index=np.arange(1),columns= k_similar_patents.columns[8:])\n",
    "        for i in range(top_n):\n",
    "            result_k_similar_patents  = result_k_similar_patents + k_similar_patents.iloc[i, 8:].values\n",
    "    #     result_k_similar_patents_r = result_k_similar_patents.reshape(1,600)\n",
    "        result_k_similar_patents_df = pd.DataFrame(result_k_similar_patents, columns= k_similar_patents.columns[8:])\n",
    "#     patent_id_input_res = pd.concat([input_patrnt_id_df, result_k_similar_patents], ignore_index=True)\n",
    "        result_k_similar_patents_df.insert(0, \"input_aptent_id\", df_claim_cpc_test.id.iloc[item], True)\n",
    "        locals()[\"predict\"+str(item)] = result_k_similar_patents_df.copy()\n",
    "        predict = pd.concat([predict, locals()[\"predict\"+str(item)]], ignore_index=True)\n",
    "        result_k_similar_patents_df = result_k_similar_patents_df[0:0]\n",
    "# predict.to_csv(r'/home/ubuntu/deeppatentsimilarity/output/new_predict_result.csv'+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), index = False)\n",
    "# creating tensor from targets_df\n",
    "#     predict = pd.read_csv('/home/ubuntu/deeppatentsimilarity/output/Multilabel17000.csv', encoding='ISO-8859-1')\n",
    "    data = torch.tensor((predict.to_numpy()).astype(float), dtype=torch.float32)\n",
    "\n",
    "# printing out result\n",
    "# print(torch_tensor)\n",
    "    m = nn.Sigmoid()\n",
    "    \n",
    "# input = torch.randn(2)\n",
    "    output = m(data)\n",
    "    output = (output>0.9).float()\n",
    "    output_df = pd.DataFrame(output, columns=predict.columns).astype(float)\n",
    "    \n",
    "    y_pred = output_df.iloc[:, :-1].to_numpy()\n",
    "    y_true = df_claim_cpc_test.iloc[:100, 6:].to_numpy()\n",
    "    result = pd.concat([output_df, df_claim_cpc_test], axis=1, ignore_index=True)\n",
    "#     result.to_csv(r'/home/ubuntu/storage_data/output/new_predict_result'+str(k)+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")+'.csv', index = False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    F1Measure_list.append(F1Measure(y_true,y_pred))\n",
    "    Recall_list.append(Recall(y_true,y_pred))\n",
    "    Accuracy_list.append(Accuracy(y_true, y_pred))\n",
    "    Precision_list.append(Precision(y_true,y_pred))\n",
    "    Hamming_Loss_list.append(Hamming_Loss(y_true, y_pred))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Runtime of the program is {end - start}\")\n",
    "    print(\"F1Measure: \", F1Measure_list[top_n])\n",
    "    print(\"Recall: \", Recall_list[top_n])\n",
    "    print(\"Accuracy: \", Accuracy_list[top_n])\n",
    "    print(\"Precision: \", Precision_list[top_n])\n",
    "    print(\"Hamming_Loss: \", Hamming_Loss_list[top_n])\n",
    "    \n",
    "output_d_metrics = {'F1Measure':F1Measure_list,'Recall_list':Recall_list, 'Accuracy_list':Accuracy_list,'Precision_list':Precision_list,'Hamming_Loss_list':Hamming_Loss_list}\n",
    "output_df_metrics = pd.DataFrame(output_d_metrics)\n",
    "# output_df_metrics.to_csv(r'/home/ubuntu/storage_data_new/output/output_df_metrics'+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
