{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util\n",
    "from sentence_transformers.datasets import SentenceLabelDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import scipy.spatial\n",
    "import numpy as np\n",
    "import os, json\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tokenizers import Tokenizer\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "torch.manual_seed(1)\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "top_claim_order = []\n",
    "top_claim_ids = []\n",
    "top_similarity_scores = []\n",
    "text_list = []\n",
    "id_list = []\n",
    "cosine_similarity_list = []\n",
    "save_F1_list = []\n",
    "finetune_score_list = []\n",
    "text_list_min = []\n",
    "id_list_min = []\n",
    "cosine_similarity_list_min = []\n",
    "save_F1_list_min = []\n",
    "finetune_score_list_min = []\n",
    "df_claim_cpc_test_list = []\n",
    "df_claim_cpc_id_list = []\n",
    "\n",
    "\n",
    "#Load sentences & embeddings from disc\n",
    "with open('/home/ubuntu/storage_data_new/data_old_strato_volume/patent_data_root/df_claim_test_1M_pre_duplicates_removed_663.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_patent_test_embeddings_id = stored_data['patent_id']\n",
    "    test_embeddings = stored_data['claim_embeddings']\n",
    "\n",
    "with open('/home/ubuntu/storage_data_new/data_old_strato_volume/patent_data_root/df_claim_train_1M_pre_duplicates_removed_663.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_patent_train_embeddings_id = stored_data['patent_id']\n",
    "    claim_embeddings = stored_data['claim_embeddings']\n",
    "    \n",
    "df_claim_cpc_test = pd.read_csv('/home/ubuntu/storage_data_new/data_old_strato_volume/patent_data_root/df_claim_test_1M_pre_duplicates_removed_663.csv', encoding='ISO-8859-1')\n",
    "df_claim_cpc_train = pd.read_csv('/home/ubuntu/storage_data_new/data_old_strato_volume/patent_data_root/df_claim_train_1M_pre_duplicates_removed_663.csv', encoding='ISO-8859-1')\n",
    "\n",
    "claims = list(df_claim_cpc_train.text)\n",
    "patent_id = list(df_claim_cpc_train.id)\n",
    "\n",
    "\n",
    "text_list = []\n",
    "id_list = []\n",
    "cosine_similarity_list = []\n",
    "save_F1_list = []\n",
    "finetune_score_list = []\n",
    "df_claim_cpc_test_list = []\n",
    "df_claim_cpc_id_list = []\n",
    "\n",
    "def get_search_hit (tet_embeddings):\n",
    "    top_claim_order = []\n",
    "    top_claim_ids = []\n",
    "    top_similarity_scores = []\n",
    "    search_hits = util.semantic_search(np.array(tet_embeddings).reshape(1,-1), claim_embeddings, 10000, 5000000, 10)\n",
    "    for item in range(len(search_hits[0])):\n",
    "        top_claim_order = search_hits[0][item].get('corpus_id')\n",
    "        top_claim_ids.append(stored_patent_train_embeddings_id[top_claim_order])\n",
    "        top_similarity_scores.append(search_hits[0][item].get('score'))    \n",
    "        top_100_similar_patents_df = pd.DataFrame({\n",
    "           'top_claim_ids': top_claim_ids,\n",
    "           'cosine_similarity': top_similarity_scores,\n",
    "        })\n",
    "    return top_100_similar_patents_df\n",
    "\n",
    "for patent_id in range(len(test_embeddings[:100])):\n",
    "    top_100_similar_patents_df = get_search_hit(test_embeddings[patent_id])\n",
    "    result = pd.DataFrame()\n",
    "    result = pd.merge(top_100_similar_patents_df, df_claim_cpc_train, left_on='top_claim_ids',right_on='id',how='left',suffixes=('_left','_right'))\n",
    "    top_100_similar_patents_df = pd.DataFrame(columns = top_100_similar_patents_df.columns)\n",
    "    save_F1 = []\n",
    "    \n",
    "\n",
    "    for item in range(len(result)):\n",
    "        data = torch.tensor((result.iloc[item:item+1, 7:].to_numpy()).astype(float), dtype=torch.float32)\n",
    "        output_df = pd.DataFrame(data, columns=result.columns[7:]).astype(float)\n",
    "        y_pred = output_df.iloc[:, :].to_numpy()\n",
    "        y_true = df_claim_cpc_test.iloc[patent_id:patent_id + 1, 5:].to_numpy()\n",
    "        temp = 0\n",
    "        for i in range(y_true.shape[0]):\n",
    "            if (sum(y_true[i]) == 0) and (sum(y_pred[i]) == 0):\n",
    "                continue\n",
    "            temp_save = (2*sum(np.logical_and(y_true[i], y_pred[i])))/ (sum(y_true[i])+sum(y_pred[i]))\n",
    "            save_F1.append(temp_save)\n",
    "            temp += temp_save\n",
    "    result['save_F1'] = save_F1\n",
    "    result['finetune_score'] = save_F1\n",
    "    for item in range(len(result)):\n",
    "        if result['save_F1'][item] >= 0.80:\n",
    "            if result['cosine_similarity'][item] * 1.2 > 1:\n",
    "                result['finetune_score'][item] = 1\n",
    "            else:\n",
    "                result['finetune_score'][item] = result['cosine_similarity'][item] * 1.2\n",
    "        if result['save_F1'][item] > 0.60 and result['save_F1'][item] < 0.80:\n",
    "            if result['cosine_similarity'][item] * 1.1 > 1:\n",
    "                result['finetune_score'][item] = 1\n",
    "            else:\n",
    "                result['finetune_score'][item] = result['cosine_similarity'][item] * 1.1\n",
    "        if result['save_F1'][item] > 0 and result['save_F1'][item] <= 0.60:\n",
    "            result['finetune_score'][item] = result['cosine_similarity'][item]\n",
    "        if result['save_F1'][item] == 0:\n",
    "            result['finetune_score'][item] = result['cosine_similarity'][item] * 0.7\n",
    "\n",
    "        \n",
    "    df_claim_cpc_test_list.append(df_claim_cpc_test['text'][patent_id])\n",
    "    df_claim_cpc_id_list.append(df_claim_cpc_test['id'][patent_id])\n",
    "    text_list.append(result['text'][result['finetune_score'].idxmax()])\n",
    "    id_list.append(result['id'][result['finetune_score'].idxmax()])\n",
    "    cosine_similarity_list.append(result['cosine_similarity'][result['finetune_score'].idxmax()])\n",
    "    save_F1_list.append(result['save_F1'][result['finetune_score'].idxmax()])\n",
    "    finetune_score_list.append(result['finetune_score'].max())\n",
    "    text_list_min.append(result['text'][result['finetune_score'].idxmin()])\n",
    "    id_list_min.append(result['id'][result['finetune_score'].idxmin()])\n",
    "    cosine_similarity_list_min.append(result['cosine_similarity'][result['finetune_score'].idxmin()])\n",
    "    save_F1_list_min.append(result['save_F1'][result['finetune_score'].idxmin()])\n",
    "    finetune_score_list_min.append(result['finetune_score'].min())\n",
    "\n",
    "    \n",
    "df_top_1_patents_finetuning = pd.DataFrame({\n",
    "           'claim_1': df_claim_cpc_test_list,\n",
    "           'claim_1_id': df_claim_cpc_id_list,\n",
    "           'claim_2': text_list,\n",
    "           'claim_2_id': id_list,\n",
    "           'cosine_similarity_2': cosine_similarity_list,\n",
    "           'save_F1_lis_2': save_F1_list,\n",
    "           'finetune_score_list_2': finetune_score_list,\n",
    "           'claim_3': text_list_min,\n",
    "           'claim_3_id': id_list_min,\n",
    "           'cosine_similarity_3': cosine_similarity_list_min,\n",
    "           'save_F1_lis_3': save_F1_list_min,\n",
    "           'finetune_score_list_3': finetune_score_list_min\n",
    "        })\n",
    "df_top_1_patents_finetuning = df_top_1_patents_finetuning[df_top_1_patents_finetuning['save_F1_lis_3']==0]\n",
    "\n",
    "\n",
    "def triplets_from_labeled_dataset(df_top_1_patents_finetuning):\n",
    "    triplets = []\n",
    "    for instance in range(len(df_top_1_patents_finetuning)):\n",
    "        triplets.append(InputExample(texts=[df_top_1_patents_finetuning.claim_1[0], df_top_1_patents_finetuning.claim_2[0], df_top_1_patents_finetuning.claim_3[0]]))\n",
    "    return triplets\n",
    "\n",
    "\n",
    "\n",
    "# You can specify any huggingface/transformers pre-trained model here, for example, bert-base-uncased, roberta-base, xlm-roberta-base\n",
    "model_name = 'flax-sentence-embeddings/all_datasets_v3_mpnet-base'\n",
    "\n",
    "### Create a torch.DataLoader that passes training batch instances to our model\n",
    "train_batch_size = 32\n",
    "output_path = (\n",
    "    \"/home/ubuntu/storage_data_new/data_old_strato_volume/finetuningPatentSBERT/model_test/finetune-batch-hard-triple-new-2022-\"\n",
    "    + model_name\n",
    "    + \"-\"\n",
    "    + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")\n",
    "num_epochs = 2\n",
    "\n",
    "logging.info(\"Loading dataset\")\n",
    "train_set = triplets_from_labeled_dataset(df_top_1_patents_finetuning)\n",
    "\n",
    "# We create a special dataset \"SentenceLabelDataset\" to wrap out train_set\n",
    "# It will yield batches that contain at least two samples with the same label\n",
    "train_data_sampler = SentenceLabelDataset(train_set)\n",
    "train_dataloader = DataLoader(train_data_sampler, batch_size=32)\n",
    "\n",
    "\n",
    "# Load pretrained model\n",
    "logging.info(\"Load model\")\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "### Triplet losses ####################\n",
    "### There are 4 triplet loss variants:\n",
    "### - BatchHardTripletLoss\n",
    "### - BatchHardSoftMarginTripletLoss\n",
    "### - BatchSemiHardTripletLoss\n",
    "### - BatchAllTripletLoss\n",
    "#######################################\n",
    "\n",
    "\n",
    "train_loss = losses.BatchHardSoftMarginTripletLoss(model=model)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n",
    "model.save(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
